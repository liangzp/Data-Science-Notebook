# Kaggle Baseline

Here is table of contents in this notebook:
- [Import Libraries and Data Input](#Import-Libraries-and-Data-Input)
- [Data Cleaning](#Data-Cleaning)
- [Data Visualization](#Data-Visualization)
   -  [Total Item Sold Transition](#Total-Item-Sold-Transition)
   -  [Item Sold in each day type](#Item-Sold-in-each-day-type)
   -  [Item sold in each State and Store](#Item-sold-in-each-State-and-Store)
   -  [Item Sold relation Analysis](#Item-Sold-relation-Analysis)
   -  [Store Analysis](#Store-Analysis)
   -  [Snap Purchase Analysis](#Snap-Purchase-Analysis)
   -  [Event Pattern Analysis](#Event-Pattern-Analysis)
   -  [One Item Features Analysis](#One-Item-Features-Analysis)

  
- [Summary](#Summary)
- [Future Work](#Future-Work)
- [References](#References)

## 导入必要的库


```python
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import seaborn as sns

import gc
import lightgbm as lgb
import time
import datetime
import xgboost as xgb
import time
import itertools
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score

%matplotlib inline
sns.set()
```

## 查看数据文件
```python
import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))
```

    /kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv
    /kaggle/input/m5-forecasting-accuracy/sample_submission.csv
    /kaggle/input/m5-forecasting-accuracy/calendar.csv
    /kaggle/input/m5-forecasting-accuracy/sell_prices.csv


**导入数据文件**

```python
INPUT_DIR = '/kaggle/input/m5-forecasting-accuracy'

calendar_df = pd.read_csv(f"{INPUT_DIR}/calendar.csv")
sell_prices_df = pd.read_csv(f"{INPUT_DIR}/sell_prices.csv")
sales_train_validation_df = pd.read_csv(f"{INPUT_DIR}/sales_train_validation.csv")
sample_submission_df = pd.read_csv(f"{INPUT_DIR}/sample_submission.csv")
```

**优化内存空间，通过转换为对应的最小占用内存的类型**

```python
# Calendar data type cast -> Memory Usage Reduction
calendar_df[["month", "snap_CA", "snap_TX", "snap_WI", "wday"]] = calendar_df[["month", "snap_CA", "snap_TX", "snap_WI", "wday"]].astype("int8")
calendar_df[["wm_yr_wk", "year"]] = calendar_df[["wm_yr_wk", "year"]].astype("int16") 
calendar_df["date"] = calendar_df["date"].astype("datetime64")

nan_features = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']
for feature in nan_features:
    calendar_df[feature].fillna('unknown', inplace = True)

calendar_df[["weekday", "event_name_1", "event_type_1", "event_name_2", "event_type_2"]] = calendar_df[["weekday", "event_name_1", "event_type_1", "event_name_2", "event_type_2"]] .astype("category")

# Sales Training dataset cast -> Memory Usage Reduction
sales_train_validation_df.loc[:, "d_1":] = sales_train_validation_df.loc[:, "d_1":].astype("int16")
```


```python
# Make ID column to sell_price dataframe
sell_prices_df.loc[:, "id"] = sell_prices_df.loc[:, "item_id"] + "_" + sell_prices_df.loc[:, "store_id"] + "_validation"
```


```python
sell_prices_df = pd.concat([sell_prices_df, sell_prices_df["item_id"].str.split("_", expand=True)], axis=1)
sell_prices_df = sell_prices_df.rename(columns={0:"cat_id", 1:"dept_id"})
sell_prices_df[["store_id", "item_id", "cat_id", "dept_id"]] = sell_prices_df[["store_id","item_id", "cat_id", "dept_id"]].astype("category")
sell_prices_df = sell_prices_df.drop(columns=2)
```

# Data Cleaning
First, let's combine all three dataframe.  
The important thing is changing data format from wide to long to make prediction model easier  
(Though this notebook doesn't dive into predicition model itself.)




```python
def make_dataframe():
    # Wide format dataset 
    df_wide_train = sales_train_validation_df.drop(columns=["item_id", "dept_id", "cat_id", "state_id","store_id", "id"]).T
    df_wide_train.index = calendar_df["date"][:1913]
    df_wide_train.columns = sales_train_validation_df["id"]
    
    # Making test label dataset
    df_wide_test = pd.DataFrame(np.zeros(shape=(56, len(df_wide_train.columns))), index=calendar_df.date[1913:], columns=df_wide_train.columns)
    df_wide = pd.concat([df_wide_train, df_wide_test])

    # Convert wide format to long format
    df_long = df_wide.stack().reset_index(1)
    df_long.columns = ["id", "value"]

    del df_wide_train, df_wide_test, df_wide
    gc.collect()
    
    df = pd.merge(pd.merge(df_long.reset_index(), calendar_df, on="date"), sell_prices_df, on=["id", "wm_yr_wk"])
    df = df.drop(columns=["d"])
#     df[["cat_id", "store_id", "item_id", "id", "dept_id"]] = df[["cat_id"", store_id", "item_id", "id", "dept_id"]].astype("category")
    df["sell_price"] = df["sell_price"].astype("float16")   
    df["value"] = df["value"].astype("int32")
    df["state_id"] = df["store_id"].str[:2].astype("category")


    del df_long
    gc.collect()

    return df

df = make_dataframe()
```


```python
df.dtypes
```




    date            datetime64[ns]
    id                      object
    value                    int32
    wm_yr_wk                 int16
    weekday               category
    wday                      int8
    month                     int8
    year                     int16
    event_name_1          category
    event_type_1          category
    event_name_2          category
    event_type_2          category
    snap_CA                   int8
    snap_TX                   int8
    snap_WI                   int8
    store_id              category
    item_id               category
    sell_price             float16
    cat_id                category
    dept_id               category
    state_id              category
    dtype: object




```python
def add_date_feature(df):
    df["year"] = df["date"].dt.year.astype("int16")
    df["month"] = df["date"].dt.month.astype("int8")
    df["week"] = df["date"].dt.week.astype("int8")
    df["day"] = df["date"].dt.day.astype("int8")
    df["quarter"]  = df["date"].dt.quarter.astype("int8")
    return df
```


```python
df = add_date_feature(df)
df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>date</th>
      <th>id</th>
      <th>value</th>
      <th>wm_yr_wk</th>
      <th>weekday</th>
      <th>wday</th>
      <th>month</th>
      <th>year</th>
      <th>event_name_1</th>
      <th>event_type_1</th>
      <th>...</th>
      <th>snap_WI</th>
      <th>store_id</th>
      <th>item_id</th>
      <th>sell_price</th>
      <th>cat_id</th>
      <th>dept_id</th>
      <th>state_id</th>
      <th>week</th>
      <th>day</th>
      <th>quarter</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2011-01-29</td>
      <td>HOBBIES_1_008_CA_1_validation</td>
      <td>12</td>
      <td>11101</td>
      <td>Saturday</td>
      <td>1</td>
      <td>1</td>
      <td>2011</td>
      <td>unknown</td>
      <td>unknown</td>
      <td>...</td>
      <td>0</td>
      <td>CA_1</td>
      <td>HOBBIES_1_008</td>
      <td>0.459961</td>
      <td>HOBBIES</td>
      <td>1</td>
      <td>CA</td>
      <td>4</td>
      <td>29</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2011-01-30</td>
      <td>HOBBIES_1_008_CA_1_validation</td>
      <td>15</td>
      <td>11101</td>
      <td>Sunday</td>
      <td>2</td>
      <td>1</td>
      <td>2011</td>
      <td>unknown</td>
      <td>unknown</td>
      <td>...</td>
      <td>0</td>
      <td>CA_1</td>
      <td>HOBBIES_1_008</td>
      <td>0.459961</td>
      <td>HOBBIES</td>
      <td>1</td>
      <td>CA</td>
      <td>4</td>
      <td>30</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2011-01-31</td>
      <td>HOBBIES_1_008_CA_1_validation</td>
      <td>0</td>
      <td>11101</td>
      <td>Monday</td>
      <td>3</td>
      <td>1</td>
      <td>2011</td>
      <td>unknown</td>
      <td>unknown</td>
      <td>...</td>
      <td>0</td>
      <td>CA_1</td>
      <td>HOBBIES_1_008</td>
      <td>0.459961</td>
      <td>HOBBIES</td>
      <td>1</td>
      <td>CA</td>
      <td>5</td>
      <td>31</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2011-02-01</td>
      <td>HOBBIES_1_008_CA_1_validation</td>
      <td>0</td>
      <td>11101</td>
      <td>Tuesday</td>
      <td>4</td>
      <td>2</td>
      <td>2011</td>
      <td>unknown</td>
      <td>unknown</td>
      <td>...</td>
      <td>0</td>
      <td>CA_1</td>
      <td>HOBBIES_1_008</td>
      <td>0.459961</td>
      <td>HOBBIES</td>
      <td>1</td>
      <td>CA</td>
      <td>5</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2011-02-02</td>
      <td>HOBBIES_1_008_CA_1_validation</td>
      <td>0</td>
      <td>11101</td>
      <td>Wednesday</td>
      <td>5</td>
      <td>2</td>
      <td>2011</td>
      <td>unknown</td>
      <td>unknown</td>
      <td>...</td>
      <td>1</td>
      <td>CA_1</td>
      <td>HOBBIES_1_008</td>
      <td>0.459961</td>
      <td>HOBBIES</td>
      <td>1</td>
      <td>CA</td>
      <td>5</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>47735392</th>
      <td>2016-06-19</td>
      <td>FOODS_3_825_WI_3_validation</td>
      <td>0</td>
      <td>11621</td>
      <td>Sunday</td>
      <td>2</td>
      <td>6</td>
      <td>2016</td>
      <td>NBAFinalsEnd</td>
      <td>Sporting</td>
      <td>...</td>
      <td>0</td>
      <td>WI_3</td>
      <td>FOODS_3_825</td>
      <td>3.980469</td>
      <td>FOODS</td>
      <td>3</td>
      <td>WI</td>
      <td>24</td>
      <td>19</td>
      <td>2</td>
    </tr>
    <tr>
      <th>47735393</th>
      <td>2016-06-18</td>
      <td>FOODS_3_826_WI_3_validation</td>
      <td>0</td>
      <td>11621</td>
      <td>Saturday</td>
      <td>1</td>
      <td>6</td>
      <td>2016</td>
      <td>unknown</td>
      <td>unknown</td>
      <td>...</td>
      <td>0</td>
      <td>WI_3</td>
      <td>FOODS_3_826</td>
      <td>1.280273</td>
      <td>FOODS</td>
      <td>3</td>
      <td>WI</td>
      <td>24</td>
      <td>18</td>
      <td>2</td>
    </tr>
    <tr>
      <th>47735394</th>
      <td>2016-06-19</td>
      <td>FOODS_3_826_WI_3_validation</td>
      <td>0</td>
      <td>11621</td>
      <td>Sunday</td>
      <td>2</td>
      <td>6</td>
      <td>2016</td>
      <td>NBAFinalsEnd</td>
      <td>Sporting</td>
      <td>...</td>
      <td>0</td>
      <td>WI_3</td>
      <td>FOODS_3_826</td>
      <td>1.280273</td>
      <td>FOODS</td>
      <td>3</td>
      <td>WI</td>
      <td>24</td>
      <td>19</td>
      <td>2</td>
    </tr>
    <tr>
      <th>47735395</th>
      <td>2016-06-18</td>
      <td>FOODS_3_827_WI_3_validation</td>
      <td>0</td>
      <td>11621</td>
      <td>Saturday</td>
      <td>1</td>
      <td>6</td>
      <td>2016</td>
      <td>unknown</td>
      <td>unknown</td>
      <td>...</td>
      <td>0</td>
      <td>WI_3</td>
      <td>FOODS_3_827</td>
      <td>1.000000</td>
      <td>FOODS</td>
      <td>3</td>
      <td>WI</td>
      <td>24</td>
      <td>18</td>
      <td>2</td>
    </tr>
    <tr>
      <th>47735396</th>
      <td>2016-06-19</td>
      <td>FOODS_3_827_WI_3_validation</td>
      <td>0</td>
      <td>11621</td>
      <td>Sunday</td>
      <td>2</td>
      <td>6</td>
      <td>2016</td>
      <td>NBAFinalsEnd</td>
      <td>Sporting</td>
      <td>...</td>
      <td>0</td>
      <td>WI_3</td>
      <td>FOODS_3_827</td>
      <td>1.000000</td>
      <td>FOODS</td>
      <td>3</td>
      <td>WI</td>
      <td>24</td>
      <td>19</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>47735397 rows × 24 columns</p>
</div>



# Data Visualization
## Total Item Sold Transition


```python
temp_series = df.groupby(["cat_id", "date"])["value"].sum()
temp_series
```




    cat_id     date      
    FOODS      2011-01-29    23178
               2011-01-30    22758
               2011-01-31    17174
               2011-02-01    18878
               2011-02-02    14603
                             ...  
    HOUSEHOLD  2016-06-15        0
               2016-06-16        0
               2016-06-17        0
               2016-06-18        0
               2016-06-19        0
    Name: value, Length: 5907, dtype: int32




```python
plt.figure(figsize=(12, 4))
plt.plot(temp_series[temp_series.index.get_level_values("cat_id") == "FOODS"].index.get_level_values("date"), temp_series[temp_series.index.get_level_values("cat_id") == "FOODS"].values, label="FOODS")
plt.plot(temp_series[temp_series.index.get_level_values("cat_id") == "HOUSEHOLD"].index.get_level_values("date"), temp_series[temp_series.index.get_level_values("cat_id") == "HOUSEHOLD"].values, label="HOUSEHOLD")
plt.plot(temp_series[temp_series.index.get_level_values("cat_id") == "HOBBIES"].index.get_level_values("date"), temp_series[temp_series.index.get_level_values("cat_id") == "HOBBIES"].values, label="HOBBIES")
plt.xlabel("Year")
plt.ylabel("# of sold items")
plt.title("Total Item Sold Transition of each Category")
plt.legend()

```

    /opt/conda/lib/python3.6/site-packages/pandas/plotting/_matplotlib/converter.py:103: FutureWarning: Using an implicitly registered datetime converter for a matplotlib plotting method. The converter was registered by pandas on import. Future versions of pandas will require you to explicitly register matplotlib converters.
    
    To register the converters:
    	>>> from pandas.plotting import register_matplotlib_converters
    	>>> register_matplotlib_converters()
      warnings.warn(msg, FutureWarning)





    <matplotlib.legend.Legend at 0x7f1d1922e320>




![png](let-s-start-from-here-beginners-data-analysis_files/let-s-start-from-here-beginners-data-analysis_16_2.png)


**Point of the graph**
1. FOODS is the most sold item category of these three categories.  
   HOUSEHOLD is the 2nd one, and HOBBIES are the least sold one.  


2. FOODS category appearently has some periodical feature.   
   During one year, it seems more items are sold in summer than in winter, however, we have to verify this.  
   As for more short time interval, it seems the trend has monthly or weekly features. (Let's take a look below)

3. HOUSEHOLD category items sold is gradually increasing from 2011.  
   However, it may be because some items are not in the store in 2011.  
   So we have to take the total item in the store into account.
   Periodical Features are not so clear in this category compared to FOODS.
  
4. In HOBBIES category, periodical features are less appearent like HOUSEHOLD category.

5. In some point (around the end of year), all categories don't have any sold.  So I think we have to consider whether we take these days into account when training models.

So let's take a look at the latest year, 2015!


```python
temp_series = temp_series.loc[temp_series.index.get_level_values("date") >= "2015-01-01"]
plt.figure(figsize=(12, 4))
plt.plot(temp_series[temp_series.index.get_level_values("cat_id") == "FOODS"].index.get_level_values("date"), temp_series[temp_series.index.get_level_values("cat_id") == "FOODS"].values, label="FOODS")
plt.plot(temp_series[temp_series.index.get_level_values("cat_id") == "HOUSEHOLD"].index.get_level_values("date"), temp_series[temp_series.index.get_level_values("cat_id") == "HOUSEHOLD"].values, label="HOUSEHOLD")
plt.plot(temp_series[temp_series.index.get_level_values("cat_id") == "HOBBIES"].index.get_level_values("date"), temp_series[temp_series.index.get_level_values("cat_id") == "HOBBIES"].values, label="HOBBIES")
plt.xlabel("Year-Month")
plt.ylabel("# of sold items")
plt.title("Total Item Sold Transition of each Category from 2015")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7f1d6f8d6208>




![png](let-s-start-from-here-beginners-data-analysis_files/let-s-start-from-here-beginners-data-analysis_18_1.png)


1. In all categories, the periodical trends is seemed weekly.  
   In previous graph, we can't easily recognize that HOUSEHOLD and HOBBIES have weekly features, but in this graph we can.
   
2. The day when all item sold is 0 is seemed to be Christmas Day, not new year's day, confirm it below.


```python
# Plot only December, 2015
temp_series = temp_series.loc[(temp_series.index.get_level_values("date") >= "2015-12-01") & (temp_series.index.get_level_values("date") <= "2015-12-31")]
plt.figure(figsize=(12, 4))
plt.plot(temp_series[temp_series.index.get_level_values("cat_id") == "FOODS"].index.get_level_values("date"), temp_series[temp_series.index.get_level_values("cat_id") == "FOODS"].values, label="FOODS")
plt.plot(temp_series[temp_series.index.get_level_values("cat_id") == "HOUSEHOLD"].index.get_level_values("date"), temp_series[temp_series.index.get_level_values("cat_id") == "HOUSEHOLD"].values, label="HOUSEHOLD")
plt.plot(temp_series[temp_series.index.get_level_values("cat_id") == "HOBBIES"].index.get_level_values("date"), temp_series[temp_series.index.get_level_values("cat_id") == "HOBBIES"].values, label="HOBBIES")
plt.xlabel("Year")
plt.ylabel("# of sold items")
plt.title("Total sold item per day in December, 2015")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7f1d1eafa9b0>




![png](let-s-start-from-here-beginners-data-analysis_files/let-s-start-from-here-beginners-data-analysis_20_1.png)


On Christmas Day, the items sold are seemed to be 0, let's check it with *.loc* method


```python
temp_series.loc[(temp_series.index.get_level_values("date") >= "2015-12-24") & (temp_series.index.get_level_values("date") <= "2015-12-26")]
```




    cat_id     date      
    FOODS      2015-12-24    24801
               2015-12-25       13
               2015-12-26    22488
    HOBBIES    2015-12-24     3408
               2015-12-25        0
               2015-12-26     4977
    HOUSEHOLD  2015-12-24     8018
               2015-12-25        1
               2015-12-26    10912
    Name: value, dtype: int32



Some items are sold even on Christmas Day, but I think these are completely noisy values.   

Until now, we can find the items sold have something weekly fetures. So let's think this:   
**Next Question: Which day of the week is the items sold most?**

## Item Sold in each day type


```python
temp_series = df.groupby(["cat_id", "wday"])["value"].sum()
temp_series
```




    cat_id     wday
    FOODS      1       7693153
               2       7767048
               3       6206765
               4       5736838
               5       5651175
               6       5683848
               7       6351112
    HOBBIES    1       1076911
               2        976586
               3        813632
               4        777821
               5        786310
               6        786135
               7        907405
    HOUSEHOLD  1       2613785
               2       2525992
               3       1948463
               4       1776018
               5       1755251
               6       1775984
               7       2085177
    Name: value, dtype: int32




```python
plt.figure(figsize=(6, 4))
left = np.arange(1,8) 
width = 0.3
weeklabel = ["Saturday", "Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday"]    # Please Confirm df


plt.bar(left, temp_series[temp_series.index.get_level_values("cat_id") == "FOODS"].values, width=width, label="FOODS")
plt.bar(left + width, temp_series[temp_series.index.get_level_values("cat_id") == "HOUSEHOLD"].values, width=width, label="HOUSEHOLD")
plt.bar(left + width + width, temp_series[temp_series.index.get_level_values("cat_id") == "HOBBIES"].values, width=width, label="HOBBIES")
plt.legend(bbox_to_anchor=(1.01, 1.01))
plt.xticks(left, weeklabel, rotation=60)
plt.xlabel("day of week")
plt.ylabel("# of sold items")
plt.title("Total sold item in each daytype")
```




    Text(0.5, 1.0, 'Total sold item in each daytype')




![png](let-s-start-from-here-beginners-data-analysis_files/let-s-start-from-here-beginners-data-analysis_26_1.png)


1. As we can probraly guess, Saturday or Sunday is the day which the items are most sold.  
   Tuesday or Wednesday is the least sold days.  
   -> Later, we visualize these correlation factors with heatmap. Looking forward to it!

2. HOBBIES are not so day dependent compared to FOODS or HOUSEHOLD.  

## Item sold in each State and Store


```python
temp_series = df.groupby(["state_id", "date"])["value"].sum()
temp_series
```




    state_id  date      
    CA        2011-01-29    14195
              2011-01-30    13805
              2011-01-31    10108
              2011-02-01    11047
              2011-02-02     9925
                            ...  
    WI        2016-06-15        0
              2016-06-16        0
              2016-06-17        0
              2016-06-18        0
              2016-06-19        0
    Name: value, Length: 5907, dtype: int32




```python
plt.figure(figsize=(12, 4))
plt.plot(temp_series[temp_series.index.get_level_values("state_id") == "CA"].index.get_level_values("date"), temp_series[temp_series.index.get_level_values("state_id") == "CA"].values, label="CA")
plt.plot(temp_series[temp_series.index.get_level_values("state_id") == "TX"].index.get_level_values("date"), temp_series[temp_series.index.get_level_values("state_id") == "TX"].values, label="TX")
plt.plot(temp_series[temp_series.index.get_level_values("state_id") == "WI"].index.get_level_values("date"), temp_series[temp_series.index.get_level_values("state_id") == "WI"].values, label="WI")
plt.xlabel("Year")
plt.ylabel("# of sold items")
plt.title("Total Item Sold Transition of each State")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7f1d19207780>




![png](let-s-start-from-here-beginners-data-analysis_files/let-s-start-from-here-beginners-data-analysis_30_1.png)


1. CA is the most sold state of these three states.  
   TX and WI are not so different except for the year 2011 and 2012.
  
2. All three states have some periodical features as we've already seen in category-based item sold graph. 

First, let's focus on stores in CA.


```python
temp_series = df.groupby(["store_id", "date"])["value"].sum()

plt.figure(figsize=(12, 4))
plt.plot(temp_series[temp_series.index.get_level_values("store_id") == "CA_1"].index.get_level_values("date"), temp_series[temp_series.index.get_level_values("store_id") == "CA_1"].values, label="CA_1")
plt.plot(temp_series[temp_series.index.get_level_values("store_id") == "CA_2"].index.get_level_values("date"), temp_series[temp_series.index.get_level_values("store_id") == "CA_2"].values, label="CA_2")
plt.plot(temp_series[temp_series.index.get_level_values("store_id") == "CA_3"].index.get_level_values("date"), temp_series[temp_series.index.get_level_values("store_id") == "CA_3"].values, label="CA_3")
plt.plot(temp_series[temp_series.index.get_level_values("store_id") == "CA_4"].index.get_level_values("date"), temp_series[temp_series.index.get_level_values("store_id") == "CA_4"].values, label="CA_4")

plt.xlabel("Year")
plt.ylabel("# of sold items")
plt.title("Total Item Sold Transition of each Store in CA")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7f1dbf723da0>




![png](let-s-start-from-here-beginners-data-analysis_files/let-s-start-from-here-beginners-data-analysis_32_1.png)


1. Three stores in CA have similar amount of item sold record.  
   CA_3 has more item sold a little bit compared to others.  

2. The standard deviation of each store seems different, confirm it later.

3. From around 2015 Spring or Summer, CA_2 increased its sold record rapidly. We have to investigate the reasons.




```python
temp_series = df.groupby(["store_id", "date"])["item_id"].count()

plt.figure(figsize=(12, 4))
plt.plot(temp_series[temp_series.index.get_level_values("store_id") == "CA_1"].index.get_level_values("date"), temp_series[temp_series.index.get_level_values("store_id") == "CA_1"].values, label="CA_1")
plt.plot(temp_series[temp_series.index.get_level_values("store_id") == "CA_2"].index.get_level_values("date"), temp_series[temp_series.index.get_level_values("store_id") == "CA_2"].values, label="CA_2")
plt.plot(temp_series[temp_series.index.get_level_values("store_id") == "CA_3"].index.get_level_values("date"), temp_series[temp_series.index.get_level_values("store_id") == "CA_3"].values, label="CA_3")
plt.plot(temp_series[temp_series.index.get_level_values("store_id") == "CA_4"].index.get_level_values("date"), temp_series[temp_series.index.get_level_values("store_id") == "CA_4"].values, label="CA_4")
plt.xlabel("Year")
plt.ylabel("# of item entries")
plt.title("Total item entries in each CA stores")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7f1d69d08e80>




![png](let-s-start-from-here-beginners-data-analysis_files/let-s-start-from-here-beginners-data-analysis_34_1.png)


3. From around 2015 Spring or Summer, CA_2 increased its sold record rapidly. We have to investigate the reasons.

-> It is because item registered in CA_2 increased rapidly.  
After summer in 2015, all stores in CA have similar registered item count


```python
temp_series = df.groupby(["store_id", "date"])["value"].std()
temp_series
```




    store_id  date      
    CA_1      2011-01-29     8.051911
              2011-01-30    10.231977
              2011-01-31     5.460182
              2011-02-01     7.009885
              2011-02-02     5.811179
                              ...    
    WI_3      2016-06-15     0.000000
              2016-06-16     0.000000
              2016-06-17     0.000000
              2016-06-18     0.000000
              2016-06-19     0.000000
    Name: value, Length: 19690, dtype: float64




```python
plt.figure(figsize=(12, 4))
plt.plot(temp_series[temp_series.index.get_level_values("store_id") == "CA_1"].index.get_level_values("date"), temp_series[temp_series.index.get_level_values("store_id") == "CA_1"].values, label="CA_1")
plt.plot(temp_series[temp_series.index.get_level_values("store_id") == "CA_2"].index.get_level_values("date"), temp_series[temp_series.index.get_level_values("store_id") == "CA_2"].values, label="CA_2")
plt.plot(temp_series[temp_series.index.get_level_values("store_id") == "CA_3"].index.get_level_values("date"), temp_series[temp_series.index.get_level_values("store_id") == "CA_3"].values, label="CA_3")
plt.plot(temp_series[temp_series.index.get_level_values("store_id") == "CA_4"].index.get_level_values("date"), temp_series[temp_series.index.get_level_values("store_id") == "CA_4"].values, label="CA_4")

plt.xlabel("Year")
plt.ylabel("Standard deviation of sold items")
plt.title("Standard deviation of sold items in CA stores")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7f1d1e88b358>




![png](let-s-start-from-here-beginners-data-analysis_files/let-s-start-from-here-beginners-data-analysis_37_1.png)


1. Since CA_3 is the most sold store in CA, standard deviation of this store is also higher than others.  
   Expecially, around the end of 2011, sold item deviation gets higher than usual. 
   
 
Let's check other state, WI next!


```python
temp_series = df.groupby(["store_id", "date"])["value"].sum()

plt.figure(figsize=(12, 4))
plt.plot(temp_series[temp_series.index.get_level_values("store_id") == "WI_1"].index.get_level_values("date"), temp_series[temp_series.index.get_level_values("store_id") == "WI_1"].values, label="WI_1")
plt.plot(temp_series[temp_series.index.get_level_values("store_id") == "WI_2"].index.get_level_values("date"), temp_series[temp_series.index.get_level_values("store_id") == "WI_2"].values, label="WI_2")
plt.plot(temp_series[temp_series.index.get_level_values("store_id") == "WI_3"].index.get_level_values("date"), temp_series[temp_series.index.get_level_values("store_id") == "WI_3"].values, label="WI_3")
plt.xlabel("Year")
plt.ylabel("# of sold items")
plt.title("Total item sold in each WI stores")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7f1daeaf9c88>




![png](let-s-start-from-here-beginners-data-analysis_files/let-s-start-from-here-beginners-data-analysis_39_1.png)


1. Stores in WI have similar item sold count.  
   Before 2013, WI_3 is the most sold store in WI, but WI_2 gradually increases its proportion. (Especially around summer in 2012)
   
2. In some point, WI_1 rapidly increase its sold item count. (Around on November 2012)

-> Total Sold out count depends on the number of entries at that day.  So Now we check the total item entries in each store as we did in CA stores.


```python
temp_series = df.groupby(["store_id", "date"])["item_id"].count()

plt.figure(figsize=(12, 4))
plt.plot(temp_series[temp_series.index.get_level_values("store_id") == "WI_1"].index.get_level_values("date"), temp_series[temp_series.index.get_level_values("store_id") == "WI_1"].values, label="WI_1")
plt.plot(temp_series[temp_series.index.get_level_values("store_id") == "WI_2"].index.get_level_values("date"), temp_series[temp_series.index.get_level_values("store_id") == "WI_2"].values, label="WI_2")
plt.plot(temp_series[temp_series.index.get_level_values("store_id") == "WI_3"].index.get_level_values("date"), temp_series[temp_series.index.get_level_values("store_id") == "WI_3"].values, label="WI_3")
plt.xlabel("Year")
plt.ylabel("# of item entries")
plt.title("Total item entries in each WI stores")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7f1dbf6d5748>




![png](let-s-start-from-here-beginners-data-analysis_files/let-s-start-from-here-beginners-data-analysis_41_1.png)


As we've already seen above, the registered item count trends are different in each store.  
WI_2 increased its item register around summer in 2012, then WI_3 increased around November of that year.  

From 2013, all stores have similar trend.  Next, stores in TX states!


```python
temp_series = df.groupby(["store_id", "date"])["value"].sum()

plt.figure(figsize=(12, 4))
plt.plot(temp_series[temp_series.index.get_level_values("store_id") == "TX_1"].index.get_level_values("date"), temp_series[temp_series.index.get_level_values("store_id") == "TX_1"].values, label="TX_1")
plt.plot(temp_series[temp_series.index.get_level_values("store_id") == "TX_2"].index.get_level_values("date"), temp_series[temp_series.index.get_level_values("store_id") == "TX_2"].values, label="TX_2")
plt.plot(temp_series[temp_series.index.get_level_values("store_id") == "TX_3"].index.get_level_values("date"), temp_series[temp_series.index.get_level_values("store_id") == "TX_3"].values, label="TX_3")
plt.xlabel("Year")
plt.ylabel("Total sold item per day")
plt.title("Total item sold in each TX stores")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7f1d8a81e7f0>




![png](let-s-start-from-here-beginners-data-analysis_files/let-s-start-from-here-beginners-data-analysis_43_1.png)


1. Oops, in 2015, it seems some extreme points exist.  
   For exmaple, around Febrary, TX_2 has almost 0 item sold. (I assume this store is closed exceptionally.)
   In contrast, in one summer day of that year, TX_3 increased its total item sold exprosively.
   
2. TX_2 has most item sold especially before 2014.


```python
temp_series = df.groupby(["store_id", "date"])["item_id"].count()

plt.figure(figsize=(12, 4))
plt.plot(temp_series[temp_series.index.get_level_values("store_id") == "TX_1"].index.get_level_values("date"), temp_series[temp_series.index.get_level_values("store_id") == "TX_1"].values, label="TX_1")
plt.plot(temp_series[temp_series.index.get_level_values("store_id") == "TX_2"].index.get_level_values("date"), temp_series[temp_series.index.get_level_values("store_id") == "TX_2"].values, label="TX_2")
plt.plot(temp_series[temp_series.index.get_level_values("store_id") == "TX_3"].index.get_level_values("date"), temp_series[temp_series.index.get_level_values("store_id") == "TX_3"].values, label="TX_3")
plt.xlabel("Year")
plt.ylabel("Total item entries")
plt.title("Total item entries in each TX stores")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7f1da4a3a8d0>




![png](let-s-start-from-here-beginners-data-analysis_files/let-s-start-from-here-beginners-data-analysis_45_1.png)


Compared to other states, TX stores have similar tendency regarding registered entries.


```python
temp_series = df.groupby(["store_id", "date"])["value"].sum()
temp_series
```




    store_id  date      
    CA_1      2011-01-29    4337
              2011-01-30    4155
              2011-01-31    2816
              2011-02-01    3051
              2011-02-02    2630
                            ... 
    WI_3      2016-06-15       0
              2016-06-16       0
              2016-06-17       0
              2016-06-18       0
              2016-06-19       0
    Name: value, Length: 19690, dtype: int32




```python
# Find the day when items are sold less than 1000 of each store
# Let's take a look at TX_2 for example
temp_series.loc[(temp_series.values < 1000) & (temp_series.index.get_level_values("date") <= "2016-04-22")].loc["TX_2"]
```




    date
    2011-12-25      0
    2012-12-25      0
    2013-12-25     11
    2014-12-25      7
    2015-03-24    131
    2015-12-25      0
    Name: value, dtype: int32



1. Oops, in 2015, it seems some extreme points exist.  
   For exmaple, around Febrary, TX_2 has almost 0 item sold. (I assume this store is closed exceptionally.)
   
   -> On 2015-03-24, TX_2 has very little item sold. 


```python
# Find the day when items are sold most of each store
temp_series.groupby(["store_id"]).idxmax()
```




    store_id
    CA_1    (CA_1, 2013-08-25 00:00:00)
    CA_2    (CA_2, 2016-04-03 00:00:00)
    CA_3    (CA_3, 2013-09-08 00:00:00)
    CA_4    (CA_4, 2016-01-31 00:00:00)
    TX_1    (TX_1, 2015-06-15 00:00:00)
    TX_2    (TX_2, 2015-06-15 00:00:00)
    TX_3    (TX_3, 2015-06-15 00:00:00)
    WI_1    (WI_1, 2015-01-31 00:00:00)
    WI_2    (WI_2, 2016-04-09 00:00:00)
    WI_3    (WI_3, 2012-06-02 00:00:00)
    Name: value, dtype: object




```python
temp_series = temp_series.reset_index()
temp_series
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>store_id</th>
      <th>date</th>
      <th>value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>CA_1</td>
      <td>2011-01-29</td>
      <td>4337</td>
    </tr>
    <tr>
      <th>1</th>
      <td>CA_1</td>
      <td>2011-01-30</td>
      <td>4155</td>
    </tr>
    <tr>
      <th>2</th>
      <td>CA_1</td>
      <td>2011-01-31</td>
      <td>2816</td>
    </tr>
    <tr>
      <th>3</th>
      <td>CA_1</td>
      <td>2011-02-01</td>
      <td>3051</td>
    </tr>
    <tr>
      <th>4</th>
      <td>CA_1</td>
      <td>2011-02-02</td>
      <td>2630</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>19685</th>
      <td>WI_3</td>
      <td>2016-06-15</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19686</th>
      <td>WI_3</td>
      <td>2016-06-16</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19687</th>
      <td>WI_3</td>
      <td>2016-06-17</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19688</th>
      <td>WI_3</td>
      <td>2016-06-18</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19689</th>
      <td>WI_3</td>
      <td>2016-06-19</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>19690 rows × 3 columns</p>
</div>




```python
plt.plot(temp_series[(temp_series["store_id"] == "CA_1") & ((temp_series["date"] >= "2013-07-15") & (temp_series["date"] <= "2013-10-15"))]["date"],
         temp_series[(temp_series["store_id"] == "CA_1") & ((temp_series["date"] >= "2013-07-15") & (temp_series["date"] <= "2013-10-15"))]["value"])
plt.xticks(rotation=60)
plt.ylabel("# of sold items")
plt.xlabel("date")
plt.title("Item sold transition around its most sold day in CA_1 store")
```




    Text(0.5, 1.0, 'Item sold transition around its most sold day in CA_1 store')




![png](let-s-start-from-here-beginners-data-analysis_files/let-s-start-from-here-beginners-data-analysis_52_1.png)


# Item Sold relation Analysis
Under Construction...  
(I tried to apply Dynamic Factor Analysis and execute the codes of this tutorial, but it seems I couldn't get informative outcome:
https://www.statsmodels.org/dev/examples/notebooks/generated/statespace_dfm_coincident.html

To Whoever can understand this method more specifically, I appreciate your comments.

## Apply Dynamic Factor Analysis Trial


```python
# import statsmodels.api as sm
```


```python
# item_id = "HOBBIES_1_008"
# temp_df = df.loc[df.item_id == item_id, ["date","store_id", "value"]]
```


```python
# store_list = ["CA_1", "CA_2", "CA_3", "CA_4", "TX_1", "TX_2", "TX_3", "WI_1", "WI_2"]

# # temp_df.drop(columns="sell_price", inplace=True)
# temp_df_wide = pd.pivot_table(temp_df, index='date', columns='store_id', values="value")
# temp_df_wide.plot(figsize=(12, 4))
# plt.legend(bbox_to_anchor=(1.01, 1.01))
```


```python
# diff_cols = ["diff_" + store for store in store_list]

# for store in store_list:
#     col = "diff_" + store
#     temp_df_wide.columns = temp_df_wide.columns.add_categories(col)
#     temp_df_wide[col] = np.log(temp_df_wide[store] + 0.1).diff() * 100
    
#     std_col = "std_" + col
    
#     temp_df_wide.columns = temp_df_wide.columns.add_categories(std_col)
#     temp_df_wide[std_col] = (temp_df_wide[col] - temp_df_wide[col].mean()) / temp_df_wide[col].std()

```


```python
# std_cols = ["std_diff_" + store for store in store_list]
```


```python
# endog = temp_df_wide.loc[:, std_cols]

# # Create the model
# mod = sm.tsa.DynamicFactor(endog, k_factors=1, factor_order=3, error_order=3)
# initial_res = mod.fit(method='powell', disp=False)
# res = mod.fit(initial_res.params, disp=False)
```


```python
# print(res.summary(separate_params=False))
```


```python
# from pandas_datareader.data import DataReader

# fig, ax = plt.subplots(figsize=(13,3))

# # Plot the factor
# dates = endog.index._mpl_repr()
# ax.plot(dates, res.factors.filtered[0], label='Factor')
# ax.legend()

# # Retrieve and also plot the NBER recession indicators
# rec = DataReader('USREC', 'fred', start=temp_df_wide.index.min(), end=temp_df_wide.index.max())
# ylim = ax.get_ylim()
# ax.fill_between(dates[:-3], ylim[0], ylim[1], rec.values[:-4,0], facecolor='k', alpha=0.1);
```


```python
# This doesn't seem make sense.
# res.plot_coefficients_of_determination(figsize=(8,2));
```

# Store Analysis


```python
temp_series = df.groupby(["store_id", "cat_id"])["value"].sum()
```


```python
store_id_list_by_state = [["CA_1", "CA_2", "CA_3", "CA_4"], ["TX_1", "TX_2", "TX_3"], ["WI_1", "WI_2", "WI_3"]] 
```


```python
fig, axs = plt.subplots(3, 4, figsize=(16, 12), sharey=True) 

for row in range(len(store_id_list_by_state)):
    for col in range(len(store_id_list_by_state[row])):
        axs[row, col].bar(x=temp_series[temp_series.index.get_level_values("store_id") == store_id_list_by_state[row][col]].index.get_level_values("cat_id"),
                          height=temp_series[temp_series.index.get_level_values("store_id") == store_id_list_by_state[row][col]].values,
                         color=["orange", "green", "blue"], label=["FOODS", "HOBBIES", "HOUSEHOLD"])
        axs[row, col].set_title(store_id_list_by_state[row][col])
        axs[row, col].set_ylabel("# of items")

fig.suptitle("Each category item sold in each store")
```




    Text(0.5, 0.98, 'Each category item sold in each store')




![png](let-s-start-from-here-beginners-data-analysis_files/let-s-start-from-here-beginners-data-analysis_67_1.png)



```python
fig, axs = plt.subplots(3, 4, figsize=(16, 12), sharey=True) 

for row in range(len(store_id_list_by_state)):
    for col in range(len(store_id_list_by_state[row])):
        axs[row, col].bar(x=temp_series[temp_series.index.get_level_values("store_id") == store_id_list_by_state[row][col]].index.get_level_values("cat_id"),
                          height=temp_series[temp_series.index.get_level_values("store_id") == store_id_list_by_state[row][col]].values / temp_series[temp_series.index.get_level_values("store_id") == store_id_list_by_state[row][col]].sum(),
                         color=["orange", "green", "blue"], label=["FOODS", "HOBBIES", "HOUSEHOLD"])
        axs[row, col].set_title(store_id_list_by_state[row][col])
        axs[row, col].set_ylabel("% of each category")

fig.suptitle("Each category item sold percentage in each store")
```




    Text(0.5, 0.98, 'Each category item sold percentage in each store')




![png](let-s-start-from-here-beginners-data-analysis_files/let-s-start-from-here-beginners-data-analysis_68_1.png)



```python
cat_id = "FOODS"

temp_series = df.groupby(["store_id", "cat_id", "wday"])["value"].sum()
temp_series = temp_series[temp_series.index.get_level_values("cat_id") == cat_id]
temp_series
```




    store_id  cat_id  wday
    CA_1      FOODS   1       966112
                      2       990509
                      3       735141
                      4       647962
                      5       631624
                               ...  
    WI_3      FOODS   3       634632
                      4       608954
                      5       590285
                      6       598550
                      7       684701
    Name: value, Length: 70, dtype: int32




```python
weekday = ["Sat", "Sun", "Mon", "Tue", "Wed", "Thu", "Fri"]
```


```python
# Combine all these three figures.
cat_list = ["FOODS", "HOBBIES", "HOUSEHOLD"]
color_list = ["orange", "green", "blue"]
temp_series = df.groupby(["store_id", "cat_id", "wday"])["value"].sum()
width = 0.25

fig, axs = plt.subplots(3, 4, figsize=(20, 12), sharey=True) 

for row in range(len(store_id_list_by_state)):
    for col in range(len(store_id_list_by_state[row])):
        for i, cat in enumerate(cat_list):
            height_numerator = temp_series[(temp_series.index.get_level_values("cat_id") == cat) & (temp_series.index.get_level_values("store_id") == store_id_list_by_state[row][col])].values
            height_denominater = height_numerator.sum()

            axs[row, col].bar(x=temp_series[(temp_series.index.get_level_values("cat_id") == cat) & (temp_series.index.get_level_values("store_id") == store_id_list_by_state[row][col])].index.get_level_values("wday") + width * (i-1),
                              height=height_numerator / height_denominater,
                             tick_label=weekday, color=color_list[i], width=width, label=cat)
            axs[row, col].set_title(store_id_list_by_state[row][col])
            axs[row, col].legend()
            
fig.suptitle("HOBBIES item sold in each store in each day")
```




    Text(0.5, 0.98, 'HOBBIES item sold in each store in each day')




![png](let-s-start-from-here-beginners-data-analysis_files/let-s-start-from-here-beginners-data-analysis_71_1.png)


# Snap Purchase Analysis
Let's see how snap purchase allowed day is distributed.


```python
fig, axs = plt.subplots(1, 3, sharey=True)
fig.suptitle("Snap Purchase Enable Day Count of each store")

sns.countplot(x="snap_CA", data =calendar_df, ax=axs[0])
sns.countplot(x="snap_TX", data =calendar_df, ax=axs[1])
sns.countplot(x="snap_WI", data =calendar_df, ax=axs[2])
```




    <matplotlib.axes._subplots.AxesSubplot at 0x7f1da3ca6048>




![png](let-s-start-from-here-beginners-data-analysis_files/let-s-start-from-here-beginners-data-analysis_73_1.png)


1. OK, the total count of snap purchase enable day looks similar in these three stores.

2. The total count of snap purchase enable day is about one half of that of non-enable day.

Next Let's see whether snap purchase is how-distributed in one year.


```python
temp_df = calendar_df.groupby(["year"])[["snap_CA", "snap_TX", "snap_WI"]].sum()
temp_df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>snap_CA</th>
      <th>snap_TX</th>
      <th>snap_WI</th>
    </tr>
    <tr>
      <th>year</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2011</th>
      <td>110</td>
      <td>110</td>
      <td>110</td>
    </tr>
    <tr>
      <th>2012</th>
      <td>120</td>
      <td>120</td>
      <td>120</td>
    </tr>
    <tr>
      <th>2013</th>
      <td>120</td>
      <td>120</td>
      <td>120</td>
    </tr>
    <tr>
      <th>2014</th>
      <td>120</td>
      <td>120</td>
      <td>120</td>
    </tr>
    <tr>
      <th>2015</th>
      <td>120</td>
      <td>120</td>
      <td>120</td>
    </tr>
    <tr>
      <th>2016</th>
      <td>60</td>
      <td>60</td>
      <td>60</td>
    </tr>
  </tbody>
</table>
</div>



1. OK. Total snap purchase allowed day of each state is the same in all years.

2. From 2011 to 2015, there are about 120 days when snap purchase is allowed.  
   (As for 2016, we only have the first half of whole year.)


```python
# This cell is just visuallizing the above dataframe.
plt.bar(temp_df.index, temp_df.snap_CA)
plt.ylabel("# of snap purchase allowed day")
plt.xlabel("Year")
plt.title("Snap Purchase allowed day yearly transition")
```




    Text(0.5, 1.0, 'Snap Purchase allowed day yearly transition')




![png](let-s-start-from-here-beginners-data-analysis_files/let-s-start-from-here-beginners-data-analysis_77_1.png)


OK, total count in one year is almost the same in all years and all states.
How about monthly distribution?


```python
temp_df = calendar_df[calendar_df["year"] == 2015].groupby(["month"])[["snap_CA", "snap_TX", "snap_WI"]].sum()
temp_df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>snap_CA</th>
      <th>snap_TX</th>
      <th>snap_WI</th>
    </tr>
    <tr>
      <th>month</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>10</td>
      <td>10</td>
      <td>10</td>
    </tr>
    <tr>
      <th>2</th>
      <td>10</td>
      <td>10</td>
      <td>10</td>
    </tr>
    <tr>
      <th>3</th>
      <td>10</td>
      <td>10</td>
      <td>10</td>
    </tr>
    <tr>
      <th>4</th>
      <td>10</td>
      <td>10</td>
      <td>10</td>
    </tr>
    <tr>
      <th>5</th>
      <td>10</td>
      <td>10</td>
      <td>10</td>
    </tr>
    <tr>
      <th>6</th>
      <td>10</td>
      <td>10</td>
      <td>10</td>
    </tr>
    <tr>
      <th>7</th>
      <td>10</td>
      <td>10</td>
      <td>10</td>
    </tr>
    <tr>
      <th>8</th>
      <td>10</td>
      <td>10</td>
      <td>10</td>
    </tr>
    <tr>
      <th>9</th>
      <td>10</td>
      <td>10</td>
      <td>10</td>
    </tr>
    <tr>
      <th>10</th>
      <td>10</td>
      <td>10</td>
      <td>10</td>
    </tr>
    <tr>
      <th>11</th>
      <td>10</td>
      <td>10</td>
      <td>10</td>
    </tr>
    <tr>
      <th>12</th>
      <td>10</td>
      <td>10</td>
      <td>10</td>
    </tr>
  </tbody>
</table>
</div>



Through the year, we have 10 snap purchase allowed days in one month.  
This tendency is the same from 2012 to 2015.  
(In 2011, no snap days in January)


```python
# Just visualizing the above dataframe
plt.bar(temp_df.index, temp_df.snap_CA)
plt.ylabel("# of snap purchase allowed day")
plt.xlabel("Month")
plt.title("Snap Purchase allowed day monthly trend")
```




    Text(0.5, 1.0, 'Snap Purchase allowed day monthly trend')




![png](let-s-start-from-here-beginners-data-analysis_files/let-s-start-from-here-beginners-data-analysis_81_1.png)


OK, total count in one month is the same through the whole year.
How about weekly distribution?


```python
temp_df = calendar_df[calendar_df["year"] == 2015].groupby(["weekday"])[["snap_CA", "snap_TX", "snap_WI"]].sum()
temp_df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>snap_CA</th>
      <th>snap_TX</th>
      <th>snap_WI</th>
    </tr>
    <tr>
      <th>weekday</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Friday</th>
      <td>17</td>
      <td>17</td>
      <td>16</td>
    </tr>
    <tr>
      <th>Monday</th>
      <td>17</td>
      <td>17</td>
      <td>18</td>
    </tr>
    <tr>
      <th>Saturday</th>
      <td>16</td>
      <td>16</td>
      <td>16</td>
    </tr>
    <tr>
      <th>Sunday</th>
      <td>17</td>
      <td>19</td>
      <td>18</td>
    </tr>
    <tr>
      <th>Thursday</th>
      <td>18</td>
      <td>18</td>
      <td>19</td>
    </tr>
    <tr>
      <th>Tuesday</th>
      <td>18</td>
      <td>17</td>
      <td>16</td>
    </tr>
    <tr>
      <th>Wednesday</th>
      <td>17</td>
      <td>16</td>
      <td>17</td>
    </tr>
  </tbody>
</table>
</div>



Regarding weekly trend, we can find no biased distribution.  
This is also almost uniformly distributed like year total and month total.


```python
plt.bar(temp_df.index, temp_df.snap_CA)
plt.xticks(rotation=60)
plt.ylabel("# of snap purchase allowed day")
plt.xlabel("Day type")
plt.title("Snap Purchase allowed day weekly trend")
```




    Text(0.5, 1.0, 'Snap Purchase allowed day weekly trend')




![png](let-s-start-from-here-beginners-data-analysis_files/let-s-start-from-here-beginners-data-analysis_85_1.png)


From above things, we may think "oh, snap_enable day is distributed uniformly, like 1 day in 3 consecutive days." because all of these barplot show it's not so different in any month, any day.  
However, it is **completely different** as I'll show you below.  
(i.e. snap purchase enable day is distributed biasedly.)


```python
# Make temp dataframe with necessary information
temp_df = df.groupby(["date", "state_id"])[["value"]].sum()
temp_df = temp_df.reset_index()
temp_df = temp_df.merge(calendar_df[["date", "snap_CA", "snap_TX", "snap_WI"]], on="date")
temp_df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>date</th>
      <th>state_id</th>
      <th>value</th>
      <th>snap_CA</th>
      <th>snap_TX</th>
      <th>snap_WI</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2011-01-29</td>
      <td>CA</td>
      <td>14195</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2011-01-29</td>
      <td>TX</td>
      <td>9438</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2011-01-29</td>
      <td>WI</td>
      <td>8998</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2011-01-30</td>
      <td>CA</td>
      <td>13805</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2011-01-30</td>
      <td>TX</td>
      <td>9630</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>5902</th>
      <td>2016-06-18</td>
      <td>TX</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5903</th>
      <td>2016-06-18</td>
      <td>WI</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5904</th>
      <td>2016-06-19</td>
      <td>CA</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5905</th>
      <td>2016-06-19</td>
      <td>TX</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5906</th>
      <td>2016-06-19</td>
      <td>WI</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>5907 rows × 6 columns</p>
</div>



Find the most item sold day for example and take a look at the relationship between snap purchase allowed flag and values.


```python
np.argmax(temp_df.groupby(["date", "state_id"])["value"].sum())
```

    /opt/conda/lib/python3.6/site-packages/numpy/core/fromnumeric.py:61: FutureWarning: 
    The current behaviour of 'Series.argmax' is deprecated, use 'idxmax'
    instead.
    The behavior of 'argmax' will be corrected to return the positional
    maximum in the future. For now, use 'series.values.argmax' or
    'np.argmax(np.array(values))' to get the position of the maximum
    row.
      return bound(*args, **kwds)





    (Timestamp('2016-03-06 00:00:00'), 'CA')




```python
temp_df = temp_df[(temp_df.date >= "2016-02-15") & (temp_df.date <= "2016-03-25") & (temp_df.state_id == "CA")]
temp_df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>date</th>
      <th>state_id</th>
      <th>value</th>
      <th>snap_CA</th>
      <th>snap_TX</th>
      <th>snap_WI</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>5529</th>
      <td>2016-02-15</td>
      <td>CA</td>
      <td>19231</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>5532</th>
      <td>2016-02-16</td>
      <td>CA</td>
      <td>15249</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5535</th>
      <td>2016-02-17</td>
      <td>CA</td>
      <td>14555</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5538</th>
      <td>2016-02-18</td>
      <td>CA</td>
      <td>15306</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5541</th>
      <td>2016-02-19</td>
      <td>CA</td>
      <td>17114</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5544</th>
      <td>2016-02-20</td>
      <td>CA</td>
      <td>21348</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5547</th>
      <td>2016-02-21</td>
      <td>CA</td>
      <td>22538</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5550</th>
      <td>2016-02-22</td>
      <td>CA</td>
      <td>16269</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5553</th>
      <td>2016-02-23</td>
      <td>CA</td>
      <td>14616</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5556</th>
      <td>2016-02-24</td>
      <td>CA</td>
      <td>13943</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5559</th>
      <td>2016-02-25</td>
      <td>CA</td>
      <td>14517</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5562</th>
      <td>2016-02-26</td>
      <td>CA</td>
      <td>16346</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5565</th>
      <td>2016-02-27</td>
      <td>CA</td>
      <td>21546</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5568</th>
      <td>2016-02-28</td>
      <td>CA</td>
      <td>23157</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5571</th>
      <td>2016-02-29</td>
      <td>CA</td>
      <td>16232</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5574</th>
      <td>2016-03-01</td>
      <td>CA</td>
      <td>16297</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5577</th>
      <td>2016-03-02</td>
      <td>CA</td>
      <td>15415</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>5580</th>
      <td>2016-03-03</td>
      <td>CA</td>
      <td>15621</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>5583</th>
      <td>2016-03-04</td>
      <td>CA</td>
      <td>16095</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5586</th>
      <td>2016-03-05</td>
      <td>CA</td>
      <td>22022</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>5589</th>
      <td>2016-03-06</td>
      <td>CA</td>
      <td>25224</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>5592</th>
      <td>2016-03-07</td>
      <td>CA</td>
      <td>17148</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5595</th>
      <td>2016-03-08</td>
      <td>CA</td>
      <td>16083</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>5598</th>
      <td>2016-03-09</td>
      <td>CA</td>
      <td>15544</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>5601</th>
      <td>2016-03-10</td>
      <td>CA</td>
      <td>15835</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5604</th>
      <td>2016-03-11</td>
      <td>CA</td>
      <td>16275</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>5607</th>
      <td>2016-03-12</td>
      <td>CA</td>
      <td>22515</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>5610</th>
      <td>2016-03-13</td>
      <td>CA</td>
      <td>21937</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5613</th>
      <td>2016-03-14</td>
      <td>CA</td>
      <td>16858</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>5616</th>
      <td>2016-03-15</td>
      <td>CA</td>
      <td>15051</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>5619</th>
      <td>2016-03-16</td>
      <td>CA</td>
      <td>14807</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5622</th>
      <td>2016-03-17</td>
      <td>CA</td>
      <td>14196</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5625</th>
      <td>2016-03-18</td>
      <td>CA</td>
      <td>16664</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5628</th>
      <td>2016-03-19</td>
      <td>CA</td>
      <td>18900</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5631</th>
      <td>2016-03-20</td>
      <td>CA</td>
      <td>19421</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5634</th>
      <td>2016-03-21</td>
      <td>CA</td>
      <td>16541</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5637</th>
      <td>2016-03-22</td>
      <td>CA</td>
      <td>15581</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5640</th>
      <td>2016-03-23</td>
      <td>CA</td>
      <td>15141</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5643</th>
      <td>2016-03-24</td>
      <td>CA</td>
      <td>15228</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5646</th>
      <td>2016-03-25</td>
      <td>CA</td>
      <td>18694</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>




```python
fig, ax1 = plt.subplots()
plt.xticks(rotation=60)
ax1.plot("date", "value", data=temp_df[temp_df.state_id == "CA"])
ax2 = ax1.twinx()  
ax2.scatter("date", "snap_CA", data=temp_df[temp_df.state_id == "CA"])
```




    <matplotlib.collections.PathCollection at 0x7f1d8d319ac8>




![png](let-s-start-from-here-beginners-data-analysis_files/let-s-start-from-here-beginners-data-analysis_91_1.png)


In above figure, each plot means whether the day allows snap purchase or not in CA.  
As you can see, snap purchase enable day is not regularly distributed like one day in three consective days.  
(ex. 2016-03-01 > 2016-03-04 > 2016-03-07 > ...)  
It is actually biasedly distributed like the figure above.  
(i.e. Snap purchase Enable Day continues from 2016-03-01 to 2016-03-10)  
And on these days, sales are also increased.  

# Event Pattern Analysis
Let's check event pattern in event_name_1 column.  
(As for event_name_2 column, there are much less non-null values compeared to event_name_1 column.


```python
plt.figure(figsize=(8, 6))
sns.countplot(x="event_type_1", data=calendar_df[calendar_df["event_name_1"] != "unknown"])
plt.xticks(rotation=90)
plt.title("Event Type Count in event name 1 column")
```




    Text(0.5, 1.0, 'Event Type Count in event name 1 column')




![png](let-s-start-from-here-beginners-data-analysis_files/let-s-start-from-here-beginners-data-analysis_94_1.png)


OK, event tyoe distributes like the graph above.   
(Most of the values are actually "unknown", but for visualization, I omitted unknown value)


```python
# Let's check the distribution of snap purchase day and event day
# Accirding to the graph, Snap CA is allowed especially when sport event occurs.

plt.figure(figsize=(8, 6))
sns.countplot(x="event_type_1", data=calendar_df[calendar_df["event_name_1"] != "unknown"], hue="snap_CA")
plt.xticks(rotation=90)
plt.legend(bbox_to_anchor=(1.01, 1.01))
plt.title("Snap Purchse allowed day Count in each event category")
```




    Text(0.5, 1.0, 'Snap Purchse allowed day Count in each event category')




![png](let-s-start-from-here-beginners-data-analysis_files/let-s-start-from-here-beginners-data-analysis_96_1.png)


Let's check the sales of event day!


```python
temp_series = df.groupby(["cat_id", "event_type_1"])["value"].mean()
temp_series
```




    cat_id     event_type_1
    FOODS      Cultural        2.000074
               National        1.777578
               Religious       1.953661
               Sporting        1.999117
               unknown         2.041676
    HOBBIES    Cultural        0.656127
               National        0.525645
               Religious       0.659141
               Sporting        0.612730
               unknown         0.690233
    HOUSEHOLD  Cultural        0.822631
               National        0.737024
               Religious       0.828819
               Sporting        0.780683
               unknown         0.880896
    Name: value, dtype: float64




```python
plt.bar(x=temp_series[temp_series.index.get_level_values("cat_id") == "HOBBIES"].index.get_level_values("event_type_1"), 
        height=temp_series[temp_series.index.get_level_values("cat_id") == "HOBBIES"].values)
plt.title("HOBBIES Item Sold mean in each event type")
plt.ylabel("Item sold mean")
plt.xlabel("Event Type")
```




    Text(0.5, 0, 'Event Type')




![png](let-s-start-from-here-beginners-data-analysis_files/let-s-start-from-here-beginners-data-analysis_99_1.png)


I thought when some cultual or sporting event occurs, HOBBIES item are more likely to be sold.  
However, this plot doesn't mean this hypothesis clearly.

## One Item Features Analysis


```python
# find out most sold item for example
df[df["value"] == df["value"].max()]
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>date</th>
      <th>id</th>
      <th>value</th>
      <th>wm_yr_wk</th>
      <th>weekday</th>
      <th>wday</th>
      <th>month</th>
      <th>year</th>
      <th>event_name_1</th>
      <th>event_type_1</th>
      <th>...</th>
      <th>snap_WI</th>
      <th>store_id</th>
      <th>item_id</th>
      <th>sell_price</th>
      <th>cat_id</th>
      <th>dept_id</th>
      <th>state_id</th>
      <th>week</th>
      <th>day</th>
      <th>quarter</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>18457320</th>
      <td>2013-09-14</td>
      <td>FOODS_3_090_CA_3_validation</td>
      <td>763</td>
      <td>11334</td>
      <td>Saturday</td>
      <td>1</td>
      <td>9</td>
      <td>2013</td>
      <td>unknown</td>
      <td>unknown</td>
      <td>...</td>
      <td>1</td>
      <td>CA_3</td>
      <td>FOODS_3_090</td>
      <td>1.0</td>
      <td>FOODS</td>
      <td>3</td>
      <td>CA</td>
      <td>37</td>
      <td>14</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
<p>1 rows × 24 columns</p>
</div>



The most sold out item in this dataseet is FOODS_3_090_CA_3_validation


```python
target_id = "FOODS_3_090_CA_3_validation"
temp_df = df[df["id"] == target_id]
temp_df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>date</th>
      <th>id</th>
      <th>value</th>
      <th>wm_yr_wk</th>
      <th>weekday</th>
      <th>wday</th>
      <th>month</th>
      <th>year</th>
      <th>event_name_1</th>
      <th>event_type_1</th>
      <th>...</th>
      <th>snap_WI</th>
      <th>store_id</th>
      <th>item_id</th>
      <th>sell_price</th>
      <th>cat_id</th>
      <th>dept_id</th>
      <th>state_id</th>
      <th>week</th>
      <th>day</th>
      <th>quarter</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>22078</th>
      <td>2011-01-29</td>
      <td>FOODS_3_090_CA_3_validation</td>
      <td>108</td>
      <td>11101</td>
      <td>Saturday</td>
      <td>1</td>
      <td>1</td>
      <td>2011</td>
      <td>unknown</td>
      <td>unknown</td>
      <td>...</td>
      <td>0</td>
      <td>CA_3</td>
      <td>FOODS_3_090</td>
      <td>1.250000</td>
      <td>FOODS</td>
      <td>3</td>
      <td>CA</td>
      <td>4</td>
      <td>29</td>
      <td>1</td>
    </tr>
    <tr>
      <th>22079</th>
      <td>2011-01-30</td>
      <td>FOODS_3_090_CA_3_validation</td>
      <td>132</td>
      <td>11101</td>
      <td>Sunday</td>
      <td>2</td>
      <td>1</td>
      <td>2011</td>
      <td>unknown</td>
      <td>unknown</td>
      <td>...</td>
      <td>0</td>
      <td>CA_3</td>
      <td>FOODS_3_090</td>
      <td>1.250000</td>
      <td>FOODS</td>
      <td>3</td>
      <td>CA</td>
      <td>4</td>
      <td>30</td>
      <td>1</td>
    </tr>
    <tr>
      <th>22080</th>
      <td>2011-01-31</td>
      <td>FOODS_3_090_CA_3_validation</td>
      <td>102</td>
      <td>11101</td>
      <td>Monday</td>
      <td>3</td>
      <td>1</td>
      <td>2011</td>
      <td>unknown</td>
      <td>unknown</td>
      <td>...</td>
      <td>0</td>
      <td>CA_3</td>
      <td>FOODS_3_090</td>
      <td>1.250000</td>
      <td>FOODS</td>
      <td>3</td>
      <td>CA</td>
      <td>5</td>
      <td>31</td>
      <td>1</td>
    </tr>
    <tr>
      <th>22081</th>
      <td>2011-02-01</td>
      <td>FOODS_3_090_CA_3_validation</td>
      <td>120</td>
      <td>11101</td>
      <td>Tuesday</td>
      <td>4</td>
      <td>2</td>
      <td>2011</td>
      <td>unknown</td>
      <td>unknown</td>
      <td>...</td>
      <td>0</td>
      <td>CA_3</td>
      <td>FOODS_3_090</td>
      <td>1.250000</td>
      <td>FOODS</td>
      <td>3</td>
      <td>CA</td>
      <td>5</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>22082</th>
      <td>2011-02-02</td>
      <td>FOODS_3_090_CA_3_validation</td>
      <td>106</td>
      <td>11101</td>
      <td>Wednesday</td>
      <td>5</td>
      <td>2</td>
      <td>2011</td>
      <td>unknown</td>
      <td>unknown</td>
      <td>...</td>
      <td>1</td>
      <td>CA_3</td>
      <td>FOODS_3_090</td>
      <td>1.250000</td>
      <td>FOODS</td>
      <td>3</td>
      <td>CA</td>
      <td>5</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>47519875</th>
      <td>2016-06-15</td>
      <td>FOODS_3_090_CA_3_validation</td>
      <td>0</td>
      <td>11620</td>
      <td>Wednesday</td>
      <td>5</td>
      <td>6</td>
      <td>2016</td>
      <td>unknown</td>
      <td>unknown</td>
      <td>...</td>
      <td>1</td>
      <td>CA_3</td>
      <td>FOODS_3_090</td>
      <td>1.599609</td>
      <td>FOODS</td>
      <td>3</td>
      <td>CA</td>
      <td>24</td>
      <td>15</td>
      <td>2</td>
    </tr>
    <tr>
      <th>47519876</th>
      <td>2016-06-16</td>
      <td>FOODS_3_090_CA_3_validation</td>
      <td>0</td>
      <td>11620</td>
      <td>Thursday</td>
      <td>6</td>
      <td>6</td>
      <td>2016</td>
      <td>unknown</td>
      <td>unknown</td>
      <td>...</td>
      <td>0</td>
      <td>CA_3</td>
      <td>FOODS_3_090</td>
      <td>1.599609</td>
      <td>FOODS</td>
      <td>3</td>
      <td>CA</td>
      <td>24</td>
      <td>16</td>
      <td>2</td>
    </tr>
    <tr>
      <th>47519877</th>
      <td>2016-06-17</td>
      <td>FOODS_3_090_CA_3_validation</td>
      <td>0</td>
      <td>11620</td>
      <td>Friday</td>
      <td>7</td>
      <td>6</td>
      <td>2016</td>
      <td>unknown</td>
      <td>unknown</td>
      <td>...</td>
      <td>0</td>
      <td>CA_3</td>
      <td>FOODS_3_090</td>
      <td>1.599609</td>
      <td>FOODS</td>
      <td>3</td>
      <td>CA</td>
      <td>24</td>
      <td>17</td>
      <td>2</td>
    </tr>
    <tr>
      <th>47691241</th>
      <td>2016-06-18</td>
      <td>FOODS_3_090_CA_3_validation</td>
      <td>0</td>
      <td>11621</td>
      <td>Saturday</td>
      <td>1</td>
      <td>6</td>
      <td>2016</td>
      <td>unknown</td>
      <td>unknown</td>
      <td>...</td>
      <td>0</td>
      <td>CA_3</td>
      <td>FOODS_3_090</td>
      <td>1.599609</td>
      <td>FOODS</td>
      <td>3</td>
      <td>CA</td>
      <td>24</td>
      <td>18</td>
      <td>2</td>
    </tr>
    <tr>
      <th>47691242</th>
      <td>2016-06-19</td>
      <td>FOODS_3_090_CA_3_validation</td>
      <td>0</td>
      <td>11621</td>
      <td>Sunday</td>
      <td>2</td>
      <td>6</td>
      <td>2016</td>
      <td>NBAFinalsEnd</td>
      <td>Sporting</td>
      <td>...</td>
      <td>0</td>
      <td>CA_3</td>
      <td>FOODS_3_090</td>
      <td>1.599609</td>
      <td>FOODS</td>
      <td>3</td>
      <td>CA</td>
      <td>24</td>
      <td>19</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>1969 rows × 24 columns</p>
</div>




```python
weekday = ["Sat", "Sun", "Mon", "Tue", "Wed", "Thu", "Fri"]

# Create one hot weekday column from wday column to calculate correlation later. 
for idx, val in enumerate(weekday):
    temp_df.loc[:, val] = (temp_df["wday"] == idx + 1).astype("int8")

temp_df
# sns.heatmap(temp_df[["value", "snap_CA", ]].corr(), annot=True)
```

    /opt/conda/lib/python3.6/site-packages/pandas/core/indexing.py:376: SettingWithCopyWarning: 
    A value is trying to be set on a copy of a slice from a DataFrame.
    Try using .loc[row_indexer,col_indexer] = value instead
    
    See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
      self.obj[key] = _infer_fill_value(value)
    /opt/conda/lib/python3.6/site-packages/pandas/core/indexing.py:494: SettingWithCopyWarning: 
    A value is trying to be set on a copy of a slice from a DataFrame.
    Try using .loc[row_indexer,col_indexer] = value instead
    
    See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
      self.obj[item] = s





<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>date</th>
      <th>id</th>
      <th>value</th>
      <th>wm_yr_wk</th>
      <th>weekday</th>
      <th>wday</th>
      <th>month</th>
      <th>year</th>
      <th>event_name_1</th>
      <th>event_type_1</th>
      <th>...</th>
      <th>week</th>
      <th>day</th>
      <th>quarter</th>
      <th>Sat</th>
      <th>Sun</th>
      <th>Mon</th>
      <th>Tue</th>
      <th>Wed</th>
      <th>Thu</th>
      <th>Fri</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>22078</th>
      <td>2011-01-29</td>
      <td>FOODS_3_090_CA_3_validation</td>
      <td>108</td>
      <td>11101</td>
      <td>Saturday</td>
      <td>1</td>
      <td>1</td>
      <td>2011</td>
      <td>unknown</td>
      <td>unknown</td>
      <td>...</td>
      <td>4</td>
      <td>29</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22079</th>
      <td>2011-01-30</td>
      <td>FOODS_3_090_CA_3_validation</td>
      <td>132</td>
      <td>11101</td>
      <td>Sunday</td>
      <td>2</td>
      <td>1</td>
      <td>2011</td>
      <td>unknown</td>
      <td>unknown</td>
      <td>...</td>
      <td>4</td>
      <td>30</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22080</th>
      <td>2011-01-31</td>
      <td>FOODS_3_090_CA_3_validation</td>
      <td>102</td>
      <td>11101</td>
      <td>Monday</td>
      <td>3</td>
      <td>1</td>
      <td>2011</td>
      <td>unknown</td>
      <td>unknown</td>
      <td>...</td>
      <td>5</td>
      <td>31</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22081</th>
      <td>2011-02-01</td>
      <td>FOODS_3_090_CA_3_validation</td>
      <td>120</td>
      <td>11101</td>
      <td>Tuesday</td>
      <td>4</td>
      <td>2</td>
      <td>2011</td>
      <td>unknown</td>
      <td>unknown</td>
      <td>...</td>
      <td>5</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22082</th>
      <td>2011-02-02</td>
      <td>FOODS_3_090_CA_3_validation</td>
      <td>106</td>
      <td>11101</td>
      <td>Wednesday</td>
      <td>5</td>
      <td>2</td>
      <td>2011</td>
      <td>unknown</td>
      <td>unknown</td>
      <td>...</td>
      <td>5</td>
      <td>2</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>47519875</th>
      <td>2016-06-15</td>
      <td>FOODS_3_090_CA_3_validation</td>
      <td>0</td>
      <td>11620</td>
      <td>Wednesday</td>
      <td>5</td>
      <td>6</td>
      <td>2016</td>
      <td>unknown</td>
      <td>unknown</td>
      <td>...</td>
      <td>24</td>
      <td>15</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>47519876</th>
      <td>2016-06-16</td>
      <td>FOODS_3_090_CA_3_validation</td>
      <td>0</td>
      <td>11620</td>
      <td>Thursday</td>
      <td>6</td>
      <td>6</td>
      <td>2016</td>
      <td>unknown</td>
      <td>unknown</td>
      <td>...</td>
      <td>24</td>
      <td>16</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>47519877</th>
      <td>2016-06-17</td>
      <td>FOODS_3_090_CA_3_validation</td>
      <td>0</td>
      <td>11620</td>
      <td>Friday</td>
      <td>7</td>
      <td>6</td>
      <td>2016</td>
      <td>unknown</td>
      <td>unknown</td>
      <td>...</td>
      <td>24</td>
      <td>17</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>47691241</th>
      <td>2016-06-18</td>
      <td>FOODS_3_090_CA_3_validation</td>
      <td>0</td>
      <td>11621</td>
      <td>Saturday</td>
      <td>1</td>
      <td>6</td>
      <td>2016</td>
      <td>unknown</td>
      <td>unknown</td>
      <td>...</td>
      <td>24</td>
      <td>18</td>
      <td>2</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>47691242</th>
      <td>2016-06-19</td>
      <td>FOODS_3_090_CA_3_validation</td>
      <td>0</td>
      <td>11621</td>
      <td>Sunday</td>
      <td>2</td>
      <td>6</td>
      <td>2016</td>
      <td>NBAFinalsEnd</td>
      <td>Sporting</td>
      <td>...</td>
      <td>24</td>
      <td>19</td>
      <td>2</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>1969 rows × 31 columns</p>
</div>




```python
# Create Event Flag (Any events occur: 1, otherwise: 0)
# Create Each Event Type Flag
temp_df.loc[:, "is_event_day"] = (temp_df["event_name_1"] != "unknown").astype("int8")
temp_df.loc[:, "is_sport_event"] = (temp_df["event_type_1"] == "Sporting").astype("int8")
temp_df.loc[:, "is_cultural_event"] = (temp_df["event_type_1"] == "Cultural").astype("int8")
temp_df.loc[:, "is_national_event"] = (temp_df["event_type_1"] == "National").astype("int8")
temp_df.loc[:, "is_religious_event"] = (temp_df["event_type_1"] == "Religious").astype("int8")

temp_df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>date</th>
      <th>id</th>
      <th>value</th>
      <th>wm_yr_wk</th>
      <th>weekday</th>
      <th>wday</th>
      <th>month</th>
      <th>year</th>
      <th>event_name_1</th>
      <th>event_type_1</th>
      <th>...</th>
      <th>Mon</th>
      <th>Tue</th>
      <th>Wed</th>
      <th>Thu</th>
      <th>Fri</th>
      <th>is_event_day</th>
      <th>is_sport_event</th>
      <th>is_cultural_event</th>
      <th>is_national_event</th>
      <th>is_religious_event</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>22078</th>
      <td>2011-01-29</td>
      <td>FOODS_3_090_CA_3_validation</td>
      <td>108</td>
      <td>11101</td>
      <td>Saturday</td>
      <td>1</td>
      <td>1</td>
      <td>2011</td>
      <td>unknown</td>
      <td>unknown</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22079</th>
      <td>2011-01-30</td>
      <td>FOODS_3_090_CA_3_validation</td>
      <td>132</td>
      <td>11101</td>
      <td>Sunday</td>
      <td>2</td>
      <td>1</td>
      <td>2011</td>
      <td>unknown</td>
      <td>unknown</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22080</th>
      <td>2011-01-31</td>
      <td>FOODS_3_090_CA_3_validation</td>
      <td>102</td>
      <td>11101</td>
      <td>Monday</td>
      <td>3</td>
      <td>1</td>
      <td>2011</td>
      <td>unknown</td>
      <td>unknown</td>
      <td>...</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22081</th>
      <td>2011-02-01</td>
      <td>FOODS_3_090_CA_3_validation</td>
      <td>120</td>
      <td>11101</td>
      <td>Tuesday</td>
      <td>4</td>
      <td>2</td>
      <td>2011</td>
      <td>unknown</td>
      <td>unknown</td>
      <td>...</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22082</th>
      <td>2011-02-02</td>
      <td>FOODS_3_090_CA_3_validation</td>
      <td>106</td>
      <td>11101</td>
      <td>Wednesday</td>
      <td>5</td>
      <td>2</td>
      <td>2011</td>
      <td>unknown</td>
      <td>unknown</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 36 columns</p>
</div>




```python
# Plot Heatmap with these columns made in previous cells
plt.figure(figsize=(14, 10))
sns.heatmap(temp_df[["value", "snap_CA", "is_event_day", "is_sport_event", "is_cultural_event", "is_national_event", "is_religious_event"] + weekday].corr(), annot=True)
plt.title("Heatmap with values, snap_CA,  event_flag and weekday columns")
```




    Text(0.5, 1.0, 'Heatmap with values, snap_CA,  event_flag and weekday columns')




![png](let-s-start-from-here-beginners-data-analysis_files/let-s-start-from-here-beginners-data-analysis_107_1.png)


We can find the following things.
1. Regarding value and other columns correlation:
   - snap purchase and other events flag has little correltion.
   - Saturday has the most positive effect on values, and Tuesday has the most negative effect.  
     (We've previously seen Saturday is the most item sold day in one week [here](#Item-Sold-in-each-day-type).)
     
2. Regarding snap_CA and weekdays columns correlation:
   - As we've previously seen, snap_CA is uniformly distributed in each day type.  
     Thus, the correlation between snap_CA and weekdays columns (ex. Monday, Tuesday, ...) are almost 0.

3. Others:
   - Looking at event and sunday correlation,it is just 0.089.  
     I thought most part of events oocur on Sunday, but it wasn't so much as I had exoected.

# Summary
   In this notebook, through some easy data visualization, we found some points regarding this dataset like below.
   1. The transition of all items sold in each category 
      - Some periodical effect. (Weekly and monthly)
      - On christmas day, there are almost no sales
   2. Which category is the most sold one?  
      - FOODS is the most sold item category of these three categories.
      - HOUSEHOLD category is the 2nd one, and the HOBBIES are the least sold one.
   3. Which day type is the most sold day? 
      - Saturday and Sunday is the most item sold day types.
      - In contrast, on weekdays like Tuesday, there are less item sold.
   4. The transition in all stores by each state
      - In CA, CA_3 store is the most sold store.  
      - In other states, not so much difference appeared.  
      - These sales trasition often corresponds to the registered item entries.
   5. Snap purchase allowed day visuaizaiton
      - The total count of Snap purchase allowed day in whole year is almost the same from 2011.
      - The total count of Snap purchase allowed day in one month is 10 in every month.
      - The total count of Snap purchase allowed day in each day type is almost uniformly distributed.
      - However, there are some biased patterns regarding snap purchase allowed flag.
        (i.e. it is not like one day in three consective days regularly, but all days in one week and none in next week)
    
   And finally we visualize some points by using heatmap.

# Future Work
If I have time, I'd like to tackle with the following things.

1. Apply Dynamic Analysis and find out the relationship among state and stores.
2. Make One item or one store prediction model for beginners like me to learn how to use lightgbm as a regressor.
3. Check out the pre-processing effect. Is that effective considering the noise samples like Christmas or other irregularly days.
4. More detailed analysis and find out some useful information for making prediction.

# References
Following notebooks are the great notebooks in this competition. 
For whom hasn't check these notebooks, I strongly recommend you to take a look at these notebooks.
(I'm sorry if I missed some other great kernels, I'll take a lookt at other notebooks if I have enough time.)

Data Visualization:

- **M5 Forecasting - Starter Data Exploration**  
  https://www.kaggle.com/robikscube/m5-forecasting-starter-data-exploration  

-  **Back to (predict) the future - Interactive M5 EDA**  
   https://www.kaggle.com/headsortails/back-to-predict-the-future-interactive-m5-eda

Making Prediction:

- **M5 - Three shades of Dark: Darker magic**  
  https://www.kaggle.com/kyakovlev/m5-three-shades-of-dark-darker-magic

-  **Very fst Model**  
   https://www.kaggle.com/ragnar123/very-fst-model

# Acknowledgment

I apologize that my english are somewhat wrong and my codes are not so beautiful one like others' codes.  
However, I tried hard to make simle codes as much as I can especially for beginners like me to learn how to use matplotlib and seaborn to do data visualization.  
Any comments or upvotes can be my very strong motivation towards much harder work! Thank you!


```python

```
