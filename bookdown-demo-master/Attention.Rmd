# 时序建模

## Attention机制
[Keras的Attention机制简单实现](https://blog.csdn.net/qq_34862636/article/details/103457982)
```{r}
library(reticulate)
use_python("/Applications/anaconda3/bin/python")
```


```{python}
import keras
import pandas 
```

### 案例1
案例介绍
让我们考虑这样一种情景: 一个序列v(假如维度是10), 里面都是数字, 我们想要预测abs(v[1:4]-v[5:8])=?, 也就是序列中, 只有6位数是有效的, 其他的数字都是随机数, 我们想要求这6位数分成的两个三位数的差的绝对值是多少。那么我们可以使用注意力机制, 它可以有效的忽略随机数的干扰, 从而提取得到有效的信息。
```{python}
import numpy as np
#from keras.models import *
#from keras.layers import Input, Dense, merge, Reshape, Flatten, RepeatVector, #Permute, Lambda, LSTM
BATCH_SIZE = 12800
TIME_STEP = 10
DEMESION = 10
OUTPUT_LEN = 3
num_sample = 300000
ONE_HOT = False
# 为了结果可重复性
np.random.seed(1337)
def create_data(n):
    '''n: 样本量'''
    X = np.zeros((n, TIME_STEP), dtype='int32')
    Y = np.zeros((n,), dtype='int32')
    Y_one_hot = np.zeros((n, OUTPUT_LEN, DEMESION), dtype='int32')
    X_one_hot = np.zeros((n, TIME_STEP, DEMESION), dtype='int32')
    for i in range(n):
        row = np.random.randint(0, 10, (TIME_STEP))
        x1 = row[1]*100+row[2]*10+row[3]
        x2 = row[5]*100+row[6]*10+row[7]
        y = abs(x1-x2)
        y_str = str(y).zfill(0)#Python zfill()方法返回指定长度的字符串，原字符串右对齐，前面填充0。
        y_digits = [int(s) for s in y_str]
        print('y_str',y_str)
        print('y_digits',y_digits)
        #将y转换为全是数字的list
        Y[i] = y
        X[i] = row
        #每一串数字转成一个二维的panel
        for c, r in enumerate(y_digits):
            Y_one_hot[i, c, r] =1
        for c,r in enumerate(row):
            X_one_hot[i, c, r] =1
    return {
        'X': X / 10, 
        'Y':Y / 1000, 
        'X_one_hot':X_one_hot,
        'Y_one_hot':Y_one_hot,
    }
create_data(3)
```
```{python,engin.path='/Applications/anaconda3/bin/python'}
from keras.models import *
from keras.layers import Input, Dense, Multiply
import keras.backend as K
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

def get_activations(model, inputs, print_shape_only=False, layer_name=None):
    # Documentation is available online on Github at the address below.
    # From: https://github.com/philipperemy/keras-visualize-activations
    print('----- activations -----')
    activations = []
    inp = model.input
    if layer_name is None:
        outputs = [layer.output for layer in model.layers]
    else:
        outputs = [layer.output for layer in model.layers if layer.name == layer_name]  # all layer outputs
    funcs = [K.function([inp] + [K.learning_phase()], [out]) for out in outputs]  # evaluation functions
    layer_outputs = [func([inputs, 1.])[0] for func in funcs]
    for layer_activations in layer_outputs:
        activations.append(layer_activations)
        if print_shape_only:
            print(layer_activations.shape)
        else:
            print(layer_activations)
    return activations
```
以上函数是可视化attention机制的参数。此处实现的attention机制很简单，如下图：
![attention结构](/Users/zhipengliang/Projects/Data-Science-Notebook/bookdown-demo-master/time-NN/20191209170800912.png)
```{python}
def get_data(n, input_dim, attention_column=1):
    """
    Data generation. x is purely random except that it's first value equals the target y.
    In practice, the network should learn that the target = x[attention_column].
    Therefore, most of its attention should be focused on the value addressed by attention_column.
    :param n: the number of samples to retrieve.
    :param input_dim: the number of dimensions of each element in the series.
    :param attention_column: the column linked to the target. Everything else is purely random.
    :return: x: model inputs, y: model targets
    """
    x = np.random.standard_normal(size=(n, input_dim))
    y = np.random.randint(low=0, high=2, size=(n, 1))
    x[:, attention_column] = y[:, 0]#将label加入到attention_column
    return x, y

def build_model():
    K.clear_session() #清除之前的模型，省得压满内存
    inputs = Input(shape=(input_dim,)) #输入层

    # ATTENTION PART STARTS HERE 注意力层
    attention_probs = Dense(input_dim, activation='softmax', name='attention_vec')(inputs)
    attention_mul =  Multiply()([inputs, attention_probs])
    # ATTENTION PART FINISHES HERE

    attention_mul = Dense(64)(attention_mul) #原始的全连接
    output = Dense(1, activation='sigmoid')(attention_mul) #输出层
    model = Model(inputs=[inputs], outputs=output)
    return model


if __name__ == '__main__':
    np.random.seed(1337)  # for reproducibility
    input_dim = 32 #特征数
    N = 10000 #数据集总记录数
    inputs_1, outputs = get_data(N, input_dim) #构造数据集

    m = build_model() #构造模型
    m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    m.summary()

    m.fit([inputs_1], outputs, epochs=20, batch_size=64, validation_split=0.2)

    testing_inputs_1, testing_outputs = get_data(1, input_dim)

    # Attention vector corresponds to the second matrix.
    # The first one is the Inputs output.
    attention_vector = get_activations(m, testing_inputs_1,
                                       print_shape_only=True,
                                       layer_name='attention_vec')[0].flatten()
    print('attention =', attention_vector)

    # plot part.


    pd.DataFrame(attention_vector, columns=['attention (%)']).plot(kind='bar',
                                                                title='Attention Mechanism as '
                                                                         'a function of input'
                                                                         ' dimensions.')
    plt.show()
```



